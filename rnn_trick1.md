# rnn中的几点tricks
## 写在前面（可以不看）

疫情自闭在家期间比较有空，正好点开nlp的技能树。
本次学习的材料是《动手学深度学习》。然而国内的翻译版质量...不尽如人意（无论是遣词还是图标，所以国内的理科生考研真的应该增加一门学术语文...至少让研究生们写中文的水平不至于落后英文一大截吧...）。

OK，作为深度学习入门（注意不是机器学习）的第二本书【注】，我推荐的是直接使用沐神的《动手学深度学习》系列视频，参考资料也可以直接使用沐神讲课用的md和pytorch/keras版本的代码示例。

本文主要解释了两个问题：
1. 为什么`detach`函数对RNN是必要的 
2. 在训练时为什么需要观察困惑度（perplexity)指标以及perplexity的实际意义


## 正文

1. 从计算图分离隐含状态

在模型训练的每次迭代中，当前批量序列的初始隐含状态来自上一个相邻批量序列的输出隐含状态。为了使模型参数的梯度计算只依赖当前的批量序列，从而减小每次迭代的计算开销，我们可以使用`detach`函数来将隐含状态从计算图分离出来。

```
def detach(state):
    if isinstance(state, (tuple, list)):
        state = [i.detach() for i in state]
    else:
        state = state.detach()
    return state
```

> 如果不分离隐含状态，则第t次的反向传播会通过$\mathbf{H}_{t-1}$ 传回第t-1次的hidden layer，造成计算量上升

* 在pytorch中形容如下：

  > `torch.Tensor` 是这个包的核心类。如果设置它的属性 `.requires_grad` 为 `True`，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用 `.backward()`，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到`.grad`属性.
  >
  > 要阻止一个张量被跟踪历史，可以调用 `.detach()` 方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。



2. 困惑度（perplexity)

  对rnn的loss，有交叉熵损失函数：

  $$\text{loss} = -\frac{1}{N} \sum_{i=1}^N \log p_{\text{target}_i}$$
  其中$N$是预测的字符总数，$p_{\text{target}_i}$是在第$i$个预测中**真实的下个字符**被预测的概率。

  而困惑度**ppl**，可以简单的认为就是对交叉熵做exp运算使得数值更好读。

  $ ppl = e^{loss}$

  > eg. 词典为[a,b,c]，下一个 gt 为 target=c，则有$p_{\text{target}_c}$ :
  >
  > 1. $p_{\text{target}_c}$ =1,完美  $ppl= 1$
  >
  > 2. $p_{\text{target}_c}$ =0，完全错误  $ppl= +∞$
  > 3. $p_{\text{target}_c}$ = 1/3，随机baseline。 $ppl= 3$
  >
  > 对于本例，只有 $ppl < 3$ ，即**困惑度小于字典容量**时可以认为模型有用

【注】
深度学习的第一本书，我无限并且只推荐 斋藤康毅 先生的《深度学习入门 基于Python的理论与实现》。学习这本书你基本上无需任何知识储备，并且在学完后可以立刻进入深度学习视觉方向论文阅读+学习框架阶段。书中关于链式法则的“苹果的例子”以及对SGD/Momentum等梯度加速方法的解释堪称经典，在最新一版中加入了BN层与resnet的介绍。
本书唯一的缺点是缺少同等深度的 rnn~lstm~词向量 知识带（即进行nlp方向的前置知识）以及迁移学习/对抗神网络的介绍。其中 nlp方向以及物体检测，迁移学习可以通过**沐神的课件**学习。而对抗神经网络恕我知识水平有限（懒）介绍不了（也算是一个大坑吧）
